# Gemmit AI Builder

**Gemmit** is a local AI-driven prototyping tool that lets you build frontend demos by chatting with a large language model (LLM). You can:

* **Write prompts** in a simple chat UI
* **Stream AI responses** in real time
* **Automatically spin up** a frontend preview server on a custom port
* **Save AI generation outputs** to a specified directory

---

## ğŸ“‚ Directory Structure

```txt
/ (project root)
â”œâ”€â”€ backend.py          # WebSocket server powering the AI chat & commands
â”œâ”€â”€ index.html          # Landing page for Gemmit (HTML/CSS/JS)
â”œâ”€â”€ chat.html           # Chat editor & preview page (HTML/CSS/JS)
â””â”€â”€ README.md           # This file
```

> If you have a separate `frontend/` folder, move `index.html` and `chat.html` into it and open that folder in your browser.

---

## ğŸ”§ Prerequisites

* **PythonÂ 3.7+**
* **gemini** CLI available (or adjust `GEMINI_PATH`)

Install required Python package:

```bash
pip install websockets
```

---

## ğŸš€ Running the Backend

From your project root, run:

```bash
# set environment variables, then start the backend
OUTPUT_DIR=./outputs \   # where AI generation files will be stored
GENERATIONS_DIR=./gens \ # (alias for OUTPUT_DIR)
GEMINI_PATH=gemini       # path to your gemini binary
python backend.py
```

* **OUTPUT\_DIR** and **GENERATIONS\_DIR** control where any files produced by your LLM process are written.
* **GEMINI\_PATH** tells the script which `gemini` executable to invoke.

The server listens onÂ `ws://localhost:8000` by default.

---

## ğŸŒ Serving the Frontend

1. Open `index.html` in your browser (e.g. drag-and-drop or via a simple HTTP server).
2. Chat UI will appear. Enter a prompt and press **Send** (or click the â¤ button).
3. Youâ€™ll be redirected to `chat.html?prompt=â€¦`, where the full chat interface lives.

**Optional:** If youâ€™d like to serve via `http-server` (Node.js), run:

```bash
npm install -g http-server
http-server . -c-1
```

Then visit `http://localhost:8080/index.html`.

---

## âš™ï¸ Configuration via `window.APP_CONFIG`

You can inject defaults into the frontend by adding a small `<script>` block before other scripts in your HTML:

```html
<script>
  window.APP_CONFIG = {
    wsUrl: 'ws://localhost:8000',  // backend URL
    previewPort: 5002,             // default port for preview iframe
    generationsDir: './outputs',    // default output folder
    conversationId: 'conv-1234'     // optional persistent ID
  };
</script>
```

---

## âœ¨ Features

* **Streaming Chat**: Real-time token streaming from the LLM.
* **Preview Pane**: Live iframe preview of your generated frontend on a custom port.
* **Start Frontend**: Send a `start-frontend` command to spin up your dev server automatically.
* **Multi-command Payloads**: Pass `generationsDir` and `context` hints to the LLM.

---

## ğŸ“– License

MIT Â© Your Name or Company

